"use strict";(globalThis.webpackChunkhumanoid_robotics=globalThis.webpackChunkhumanoid_robotics||[]).push([[370],{6996:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>o,toc:()=>d});const o=JSON.parse('{"id":"advanced-topics/translate-your-site","title":"Module 4: Vision-Language-Action (VLA)","description":"Module 4 explores the cutting-edge field of Vision-Language-Action (VLA) systems, focusing on how Large Language Models (LLMs) can be used for humanoid robot control. You will learn:","source":"@site/docs/advanced-topics/translate-your-site.md","sourceDirName":"advanced-topics","slug":"/advanced-topics/translate-your-site","permalink":"/Hakathon_01/docs/advanced-topics/translate-your-site","draft":false,"unlisted":false,"editUrl":"https://github.com/spect-kit-plus/Hakathon_01/tree/main/Humanoid_Robotics/docs/advanced-topics/translate-your-site.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Module 4: Vision-Language-Action (VLA)"},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: NVIDIA Isaac AI and Perception","permalink":"/Hakathon_01/docs/advanced-topics/manage-docs-versions"},"next":{"title":"Module 1: The Robotic Nervous System (ROS 2)","permalink":"/Hakathon_01/docs/modules/ros2/"}}');var s=t(4848),i=t(8453);const a={sidebar_position:2,title:"Module 4: Vision-Language-Action (VLA)"},r="Module 4: Vision-Language-Action (VLA)",c={},d=[];function l(e){const n={code:"code",h1:"h1",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,s.jsx)(n.p,{children:"Module 4 explores the cutting-edge field of Vision-Language-Action (VLA) systems, focusing on how Large Language Models (LLMs) can be used for humanoid robot control. You will learn:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"VLA System Architecture:"})," Understand the integration of visual perception, natural language understanding, and robot action generation to create intelligent agents."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"LLM-based Humanoid Control:"})," Explore techniques for translating natural language commands into robot actions and behaviors using advanced GPT models."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"OpenAI API Integration:"})," Integrate OpenAI's Whisper API for accurate speech-to-text conversion, enabling robots to understand spoken commands."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"ROS 2 and LLM Orchestration:"})," Use ",(0,s.jsx)(n.code,{children:"rclpy"})," to bridge the gap between ROS 2 robotic systems and LLM-based decision-making, allowing for dynamic and intelligent task execution."]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Python for LLM Integration:"})," Develop Python scripts to manage the flow of information between perception systems, LLMs, and robot actuators, especially in diverse simulation environments (Gazebo, Unity, Isaac Sim)."]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"This module prepares you to develop humanoid robots that can understand and respond to human instructions in a highly intuitive and flexible manner."})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(l,{...e})}):l(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>r});var o=t(6540);const s={},i=o.createContext(s);function a(e){const n=o.useContext(i);return o.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),o.createElement(i.Provider,{value:n},e.children)}}}]);