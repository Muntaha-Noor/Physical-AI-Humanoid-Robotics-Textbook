"use strict";(globalThis.webpackChunkhumanoid_robotics=globalThis.webpackChunkhumanoid_robotics||[]).push([[386],{8453:(e,o,n)=>{n.d(o,{R:()=>s,x:()=>c});var i=n(6540);const t={},a=i.createContext(t);function s(e){const o=i.useContext(a);return i.useMemo(function(){return"function"==typeof e?e(o):{...o,...e}},[o,e])}function c(e){let o;return o=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:s(e.components),i.createElement(a.Provider,{value:o},e.children)}},9177:(e,o,n)=>{n.r(o),n.d(o,{assets:()=>l,contentTitle:()=>c,default:()=>u,frontMatter:()=>s,metadata:()=>i,toc:()=>r});const i=JSON.parse('{"id":"modules/vla/index","title":"Module 4: Vision-Language-Action (VLA)","description":"This module integrates large language models (LLMs) with humanoid robotics for voice-driven autonomous actions.","source":"@site/docs/modules/04-vla/index.md","sourceDirName":"modules/04-vla","slug":"/modules/vla/","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/modules/vla/","draft":false,"unlisted":false,"editUrl":"https://github.com/spect-kit-plus/Hakathon_01/tree/main/Humanoid_Robotics/docs/modules/04-vla/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Nav2 Path-Planning for Humanoid Robots","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/modules/isaac-ai/chapter3"},"next":{"title":"Chapter 1: Voice-to-Action using OpenAI Whisper","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/modules/vla/chapter1"}}');var t=n(4848),a=n(8453);const s={},c="Module 4: Vision-Language-Action (VLA)",l={},r=[{value:"Documentation",id:"documentation",level:2},{value:"Chapters",id:"chapters",level:2}];function d(e){const o={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,a.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(o.header,{children:(0,t.jsx)(o.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,t.jsx)(o.p,{children:"This module integrates large language models (LLMs) with humanoid robotics for voice-driven autonomous actions."}),"\n",(0,t.jsx)(o.h2,{id:"documentation",children:"Documentation"}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsxs)(o.li,{children:["Specification and Planning documents are available in the project's ",(0,t.jsx)(o.code,{children:"specs/004-module-4-vla/"})," directory."]}),"\n"]}),"\n",(0,t.jsx)(o.h2,{id:"chapters",children:"Chapters"}),"\n",(0,t.jsxs)(o.ul,{children:["\n",(0,t.jsx)(o.li,{children:(0,t.jsx)(o.a,{href:"/Physical-AI-Humanoid-Robotics-Textbook/docs/modules/vla/chapter1",children:"Chapter 1: Voice-to-Action using OpenAI Whisper"})}),"\n",(0,t.jsx)(o.li,{children:(0,t.jsx)(o.a,{href:"/Physical-AI-Humanoid-Robotics-Textbook/docs/modules/vla/chapter2",children:"Chapter 2: Cognitive Planning with LLMs"})}),"\n",(0,t.jsx)(o.li,{children:(0,t.jsx)(o.a,{href:"/Physical-AI-Humanoid-Robotics-Textbook/docs/modules/vla/chapter3",children:"Chapter 3: Capstone Autonomous Humanoid Project"})}),"\n"]})]})}function u(e={}){const{wrapper:o}={...(0,a.R)(),...e.components};return o?(0,t.jsx)(o,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}}}]);