"use strict";(globalThis.webpackChunkhumanoid_robotics=globalThis.webpackChunkhumanoid_robotics||[]).push([[386],{8453:(e,n,o)=>{o.d(n,{R:()=>s,x:()=>r});var t=o(6540);const a={},i=t.createContext(a);function s(e){const n=t.useContext(i);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(a):e.components||a:s(e.components),t.createElement(i.Provider,{value:n},e.children)}},9177:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>r,default:()=>u,frontMatter:()=>s,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"modules/vla/index","title":"Module 4: Vision-Language-Action (VLA)","description":"This module integrates large language models (LLMs) with humanoid robotics for voice-driven autonomous actions.","source":"@site/docs/modules/04-vla/index.md","sourceDirName":"modules/04-vla","slug":"/modules/vla/","permalink":"/Hakathon_01/docs/modules/vla/","draft":false,"unlisted":false,"editUrl":"https://github.com/spect-kit-plus/Hakathon_01/tree/main/Humanoid_Robotics/docs/modules/04-vla/index.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 3: Nav2 Path-Planning for Humanoid Robots","permalink":"/Hakathon_01/docs/modules/isaac-ai/chapter3"},"next":{"title":"Chapter 1: Voice-to-Action using OpenAI Whisper","permalink":"/Hakathon_01/docs/modules/vla/chapter1"}}');var a=o(4848),i=o(8453);const s={},r="Module 4: Vision-Language-Action (VLA)",l={},c=[{value:"Documentation",id:"documentation",level:2},{value:"Chapters",id:"chapters",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",header:"header",li:"li",p:"p",ul:"ul",...(0,i.R)(),...e.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(n.header,{children:(0,a.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,a.jsx)(n.p,{children:"This module integrates large language models (LLMs) with humanoid robotics for voice-driven autonomous actions."}),"\n",(0,a.jsx)(n.h2,{id:"documentation",children:"Documentation"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsxs)(n.li,{children:["Specification and Planning documents are available in the project's ",(0,a.jsx)(n.code,{children:"specs/004-module-4-vla/"})," directory."]}),"\n"]}),"\n",(0,a.jsx)(n.h2,{id:"chapters",children:"Chapters"}),"\n",(0,a.jsxs)(n.ul,{children:["\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/Hakathon_01/docs/modules/vla/chapter1",children:"Chapter 1: Voice-to-Action using OpenAI Whisper"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/Hakathon_01/docs/modules/vla/chapter2",children:"Chapter 2: Cognitive Planning with LLMs"})}),"\n",(0,a.jsx)(n.li,{children:(0,a.jsx)(n.a,{href:"/Hakathon_01/docs/modules/vla/chapter3",children:"Chapter 3: Capstone Autonomous Humanoid Project"})}),"\n"]})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,a.jsx)(n,{...e,children:(0,a.jsx)(d,{...e})}):d(e)}}}]);