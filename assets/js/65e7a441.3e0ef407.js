"use strict";(globalThis.webpackChunkhumanoid_robotics=globalThis.webpackChunkhumanoid_robotics||[]).push([[862],{8453:(e,t,o)=>{o.d(t,{R:()=>a,x:()=>r});var n=o(6540);const i={},s=n.createContext(i);function a(e){const t=n.useContext(s);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),n.createElement(s.Provider,{value:t},e.children)}},9983:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>a,metadata:()=>n,toc:()=>u});const n=JSON.parse('{"id":"modules/vla/chapter1","title":"Chapter 1: Voice-to-Action using OpenAI Whisper","description":"","source":"@site/docs/modules/04-vla/01-chapter1.md","sourceDirName":"modules/04-vla","slug":"/modules/vla/chapter1","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/modules/vla/chapter1","draft":false,"unlisted":false,"editUrl":"https://github.com/spect-kit-plus/Hakathon_01/tree/main/Humanoid_Robotics/docs/modules/04-vla/01-chapter1.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/modules/vla/"},"next":{"title":"Chapter 2: Cognitive Planning with LLMs","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/modules/vla/chapter2"}}');var i=o(4848),s=o(8453);const a={},r="Chapter 1: Voice-to-Action using OpenAI Whisper",c={},u=[];function l(e){const t={h1:"h1",header:"header",...(0,s.R)(),...e.components};return(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"chapter-1-voice-to-action-using-openai-whisper",children:"Chapter 1: Voice-to-Action using OpenAI Whisper"})})}function d(e={}){const{wrapper:t}={...(0,s.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);