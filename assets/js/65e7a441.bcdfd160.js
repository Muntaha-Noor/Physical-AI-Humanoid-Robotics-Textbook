"use strict";(globalThis.webpackChunkhumanoid_robotics=globalThis.webpackChunkhumanoid_robotics||[]).push([[862],{8453:(e,t,o)=>{o.d(t,{R:()=>s,x:()=>r});var n=o(6540);const i={},a=n.createContext(i);function s(e){const t=n.useContext(a);return n.useMemo(function(){return"function"==typeof e?e(t):{...t,...e}},[t,e])}function r(e){let t;return t=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:s(e.components),n.createElement(a.Provider,{value:t},e.children)}},9983:(e,t,o)=>{o.r(t),o.d(t,{assets:()=>c,contentTitle:()=>r,default:()=>d,frontMatter:()=>s,metadata:()=>n,toc:()=>u});const n=JSON.parse('{"id":"modules/vla/chapter1","title":"Chapter 1: Voice-to-Action using OpenAI Whisper","description":"","source":"@site/docs/modules/04-vla/01-chapter1.md","sourceDirName":"modules/04-vla","slug":"/modules/vla/chapter1","permalink":"/Hakathon_01/docs/modules/vla/chapter1","draft":false,"unlisted":false,"editUrl":"https://github.com/spect-kit-plus/Hakathon_01/tree/main/Humanoid_Robotics/docs/modules/04-vla/01-chapter1.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Module 4: Vision-Language-Action (VLA)","permalink":"/Hakathon_01/docs/modules/vla/"},"next":{"title":"Chapter 2: Cognitive Planning with LLMs","permalink":"/Hakathon_01/docs/modules/vla/chapter2"}}');var i=o(4848),a=o(8453);const s={},r="Chapter 1: Voice-to-Action using OpenAI Whisper",c={},u=[];function l(e){const t={h1:"h1",header:"header",...(0,a.R)(),...e.components};return(0,i.jsx)(t.header,{children:(0,i.jsx)(t.h1,{id:"chapter-1-voice-to-action-using-openai-whisper",children:"Chapter 1: Voice-to-Action using OpenAI Whisper"})})}function d(e={}){const{wrapper:t}={...(0,a.R)(),...e.components};return t?(0,i.jsx)(t,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}}}]);