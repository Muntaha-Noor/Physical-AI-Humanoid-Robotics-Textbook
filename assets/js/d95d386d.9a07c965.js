"use strict";(globalThis.webpackChunkhumanoid_robotics=globalThis.webpackChunkhumanoid_robotics||[]).push([[786],{610:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>d,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>s,toc:()=>l});const s=JSON.parse('{"id":"modules/vla/chapter2","title":"Chapter 2: Building a Voice-Controlled Robot with ROS 2 and Whisper","description":"2.1 System Architecture: The Flow of Voice to Action","source":"@site/docs/modules/04-vla/02-chapter2.md","sourceDirName":"modules/04-vla","slug":"/modules/vla/chapter2","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/modules/vla/chapter2","draft":false,"unlisted":false,"editUrl":"https://github.com/spect-kit-plus/Hakathon_01/tree/main/Humanoid_Robotics/docs/modules/04-vla/02-chapter2.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"Chapter 1: The Confluence of Vision and Language in Robotics","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/modules/vla/chapter1"},"next":{"title":"Chapter 3: Advanced Task Planning with Large Language Models","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/docs/modules/vla/chapter3"}}');var i=t(4848),o=t(8453);const r={},a="Chapter 2: Building a Voice-Controlled Robot with ROS 2 and Whisper",d={},l=[{value:"2.1 System Architecture: The Flow of Voice to Action",id:"21-system-architecture-the-flow-of-voice-to-action",level:2},{value:"2.2 The Ears: Capturing Audio in ROS 2",id:"22-the-ears-capturing-audio-in-ros-2",level:2},{value:"2.3 The Translator: Speech-to-Text with Whisper",id:"23-the-translator-speech-to-text-with-whisper",level:2},{value:"2.4 The Brain: From Text to Robot Control",id:"24-the-brain-from-text-to-robot-control",level:2},{value:"2.5 Summary",id:"25-summary",level:2}];function c(e){const n={code:"code",em:"em",h1:"h1",h2:"h2",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",...(0,o.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-2-building-a-voice-controlled-robot-with-ros-2-and-whisper",children:"Chapter 2: Building a Voice-Controlled Robot with ROS 2 and Whisper"})}),"\n",(0,i.jsx)(n.h2,{id:"21-system-architecture-the-flow-of-voice-to-action",children:"2.1 System Architecture: The Flow of Voice to Action"}),"\n",(0,i.jsx)(n.p,{children:"In this chapter, we will build a complete system that allows us to control a robot using natural language voice commands. This is a classic robotics project that provides a fantastic introduction to multimodal interaction. Our system will be composed of three distinct, decoupled ROS 2 nodes. This modular architecture is a core principle of ROS and makes the system easier to debug, maintain, and expand."}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Audio Capture Node (",(0,i.jsx)(n.code,{children:"audio_capture_node.py"}),"):"]})," This node's sole responsibility is to interface with the microphone hardware. It captures raw audio data and publishes it in a continuous stream onto a ROS 2 topic."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Speech-to-Text Node (",(0,i.jsx)(n.code,{children:"whisper_service_node.py"}),"):"]}),' This node acts as the "ears" of our robot. It subscribes to the raw audio stream. To avoid sending a constant, computationally expensive stream of data to the cloud, it will implement voice activity detection (VAD) to identify when a user is actually speaking. Once speech is detected and has concluded, it will take the audio segment, send it to the OpenAI Whisper API for transcription, and publish the resulting text to a new topic.']}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:["Command Interpretation Node (",(0,i.jsx)(n.code,{children:"command_node.py"}),"):"]}),' This node is the "brain." It subscribes to the transcribed text topic. It then parses this text to understand the user\'s intent and translates that intent into a specific robot command (e.g., a ',(0,i.jsx)(n.code,{children:"geometry_msgs/Twist"})," message) which it publishes to the robot's control topic (e.g., ",(0,i.jsx)(n.code,{children:"/cmd_vel"}),")."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.p,{children:'*Placeholder for a detailed diagram: "Voice Control System Architecture." This diagram should clearly show the three nodes and the data flowing between them:'}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["A microphone icon points to the ",(0,i.jsx)(n.code,{children:"Audio Capture Node"}),"."]}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"Audio Capture Node"})," publishes an ",(0,i.jsx)(n.code,{children:"audio_msgs/Audio"})," message to an ",(0,i.jsx)(n.code,{children:"/audio"})," topic."]}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"Whisper Service Node"})," subscribes to the ",(0,i.jsx)(n.code,{children:"/audio"}),' topic. An arrow shows it communicating with a cloud icon labeled "OpenAI Whisper API".']}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"Whisper Service Node"})," then publishes a ",(0,i.jsx)(n.code,{children:"std_msgs/String"})," message to a ",(0,i.jsx)(n.code,{children:"/transcribed_text"})," topic."]}),"\n",(0,i.jsxs)(n.li,{children:["The ",(0,i.jsx)(n.code,{children:"Command Node"})," subscribes to the ",(0,i.jsx)(n.code,{children:"/transcribed_text"})," topic."]}),"\n",(0,i.jsxs)(n.li,{children:["Finally, the ",(0,i.jsx)(n.code,{children:"Command Node"})," publishes a ",(0,i.jsx)(n.code,{children:"geometry_msgs/Twist"})," message to the ",(0,i.jsx)(n.code,{children:"/cmd_vel"})," topic, which points to a robot icon.\nThis visualization makes the data pipeline clear."]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"22-the-ears-capturing-audio-in-ros-2",children:"2.2 The Ears: Capturing Audio in ROS 2"}),"\n",(0,i.jsxs)(n.p,{children:["First, we need to get audio from a microphone into our ROS 2 system. We'll use the ",(0,i.jsx)(n.code,{children:"sounddevice"})," Python library, which provides a simple interface to the PortAudio library for cross-platform audio I/O."]}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Prerequisites:"})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Install the sounddevice library and a simple VAD tool\npip install sounddevice webrtcvad\n\n# Ensure you have a microphone connected and configured on your system.\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["A More Robust ",(0,i.jsx)(n.code,{children:"audio_capture_node.py"}),":"]})}),"\n",(0,i.jsxs)(n.p,{children:["This version publishes audio in a custom message type, which is better practice than using a generic ",(0,i.jsx)(n.code,{children:"Int16MultiArray"}),"."]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"# In your ROS 2 package, create a 'msg' directory.\n# Inside, create a file named 'Audio.msg' with the following content:\n#\n# std_msgs/Header header\n# int16[] data\n# uint32 sample_rate\n#\n# Remember to update package.xml and CMakeLists.txt to build this message!\n\n# The node itself:\nimport rclpy\nfrom rclpy.node import Node\nfrom your_package_name.msg import Audio # Import the custom message\nimport sounddevice as sd\nimport numpy as np\n\nclass AudioCaptureNode(Node):\n    def __init__(self):\n        super().__init__('audio_capture_node')\n        self.publisher_ = self.create_publisher(Audio, 'audio', 10)\n        \n        # Audio parameters\n        self.sample_rate = 16000  # 16kHz is standard for speech recognition\n        self.channels = 1\n        self.block_size = int(self.sample_rate * 0.1) # 100ms chunks\n\n        self.get_logger().info(\"Starting audio stream...\")\n        self.stream = sd.InputStream(\n            callback=self.audio_callback,\n            samplerate=self.sample_rate,\n            channels=self.channels,\n            blocksize=self.block_size,\n            dtype='int16'\n        )\n        self.stream.start()\n        self.get_logger().info(\"Audio stream started.\")\n\n    def audio_callback(self, indata, frames, time, status):\n        if status:\n            self.get_logger().warn(f\"Audio callback status: {status}\")\n        \n        msg = Audio()\n        msg.header.stamp = self.get_clock().now().to_msg()\n        msg.header.frame_id = \"microphone\"\n        msg.data = indata.flatten().tolist()\n        msg.sample_rate = self.sample_rate\n        self.publisher_.publish(msg)\n\ndef main(args=None):\n    rclpy.init(args=args)\n    audio_capture_node = AudioCaptureNode()\n    try:\n        rclpy.spin(audio_capture_node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        audio_capture_node.get_logger().info(\"Stopping audio stream.\")\n        audio_capture_node.stream.stop()\n        audio_capture_node.stream.close()\n        audio_capture_node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.em,{children:["This improved node uses an ",(0,i.jsx)(n.code,{children:"InputStream"})," for a more efficient, callback-based approach and publishes audio with header information and the sample rate, which is crucial for downstream nodes."]})}),"\n",(0,i.jsx)(n.h2,{id:"23-the-translator-speech-to-text-with-whisper",children:"2.3 The Translator: Speech-to-Text with Whisper"}),"\n",(0,i.jsx)(n.p,{children:"This node is the most complex. It will listen for audio, use a Voice Activity Detector (VAD) to decide when speech is present, buffer that speech, and send it for transcription."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["The ",(0,i.jsx)(n.code,{children:"whisper_service_node.py"})," with Voice Activity Detection:"]})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom your_package_name.msg import Audio # The custom message\nimport openai\nimport numpy as np\nimport tempfile\nimport os\nfrom scipy.io.wavfile import write\nimport webrtcvad\n\nclass WhisperServiceNode(Node):\n    def __init__(self):\n        super().__init__(\'whisper_service_node\')\n        self.subscription = self.create_subscription(\n            Audio, \'audio\', self.audio_callback, 10)\n        self.publisher_ = self.create_publisher(String, \'transcribed_text\', 10)\n        \n        # VAD setup\n        self.vad = webrtcvad.Vad(3) # Aggressiveness mode 3 is the most aggressive\n        self.speech_buffer = bytearray()\n        self.is_speaking = False\n        self.silence_frames = 0\n        self.silence_threshold = 15 # Number of non-speech frames to end an utterance (1.5s)\n\n    def audio_callback(self, msg: Audio):\n        # VAD requires 16-bit PCM audio in 10, 20, or 30ms frames\n        frame_duration_ms = 1000 * len(msg.data) / msg.sample_rate\n        if frame_duration_ms not in [10.0, 20.0, 30.0, 100.0]: # Our node sends 100ms\n             # This part is simplified; a real implementation would need to chunk the audio correctly for VAD.\n             # For this example, we\'ll assume the audio_capture_node is configured to send 30ms chunks.\n             pass\n\n        audio_bytes = np.array(msg.data, dtype=np.int16).tobytes()\n        is_speech = self.vad.is_speech(audio_bytes, msg.sample_rate)\n\n        if self.is_speaking:\n            self.speech_buffer.extend(audio_bytes)\n            if not is_speech:\n                self.silence_frames += 1\n                if self.silence_frames > self.silence_threshold:\n                    self.is_speaking = False\n                    self.transcribe_buffer()\n                    self.speech_buffer = bytearray()\n            else:\n                self.silence_frames = 0 # Reset silence counter\n        elif is_speech:\n            self.get_logger().info("Speech detected, recording...")\n            self.is_speaking = True\n            self.speech_buffer.extend(audio_bytes)\n            self.silence_frames = 0\n\n    def transcribe_buffer(self):\n        self.get_logger().info("End of speech detected, transcribing...")\n        if not self.speech_buffer:\n            return\n\n        audio_np = np.frombuffer(self.speech_buffer, dtype=np.int16)\n\n        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp_audio_file:\n            write(tmp_audio_file.name, self.subscription.msg_type.sample_rate, audio_np)\n            tmp_audio_file.close()\n\n            try:\n                with open(tmp_audio_file.name, "rb") as f:\n                    transcript = openai.Audio.transcribe("whisper-1", f)\n                \n                text = transcript[\'text\']\n                if text.strip(): # Only publish if there is non-empty text\n                    text_msg = String()\n                    text_msg.data = text\n                    self.publisher_.publish(text_msg)\n                    self.get_logger().info(f\'Transcribed and published: "{text}"\')\n            except openai.APIError as e:\n                self.get_logger().error(f"OpenAI API error during transcription: {e}")\n            finally:\n                os.remove(tmp_audio_file.name)\n\n# main function as before...\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"This version is much more practical. It uses VAD to avoid transcribing silence, making it more efficient and cost-effective."})}),"\n",(0,i.jsx)(n.h2,{id:"24-the-brain-from-text-to-robot-control",children:"2.4 The Brain: From Text to Robot Control"}),"\n",(0,i.jsx)(n.p,{children:"Finally, the command node interprets the text. For now, we'll stick to simple keyword spotting, but this node is where you would integrate more advanced natural language understanding (NLU) in the future."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsxs)(n.strong,{children:["The ",(0,i.jsx)(n.code,{children:"command_node.py"})," script:"]})}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'import rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom geometry_msgs.msg import Twist\nimport time\n\nclass CommandNode(Node):\n    def __init__(self):\n        super().__init__(\'command_node\')\n        self.subscription = self.create_subscription(\n            String, \'transcribed_text\', self.text_callback, 10)\n        self.cmd_vel_pub = self.create_publisher(Twist, \'cmd_vel\', 10)\n        self.get_logger().info("Command node ready to receive text.")\n\n    def text_callback(self, msg):\n        command = msg.data.lower().strip()\n        self.get_logger().info(f\'Interpreting command: "{command}"\')\n        \n        twist_msg = Twist()\n        publish_duration = 2.0 # Publish the command for 2 seconds\n\n        if "forward" in command:\n            twist_msg.linear.x = 0.5\n        elif "backward" in command or "back" in command:\n            twist_msg.linear.x = -0.5\n        elif "turn left" in command:\n            twist_msg.angular.z = 0.5\n        elif "turn right" in command:\n            twist_msg.angular.z = -0.5\n        elif "stop" in command:\n            # Publish a zero-velocity message immediately\n            self.cmd_vel_pub.publish(Twist())\n            return\n        else:\n            self.get_logger().warn(f"Command not understood: \'{command}\'")\n            return\n\n        # Publish the command for a fixed duration\n        self.get_logger().info(f"Executing command for {publish_duration} seconds.")\n        start_time = time.time()\n        while time.time() - start_time < publish_duration:\n            self.cmd_vel_pub.publish(twist_msg)\n            time.sleep(0.1)\n        \n        # Stop the robot after the duration\n        self.cmd_vel_pub.publish(Twist())\n\n# main function as before...\n'})}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.em,{children:"This improved control node publishes the command for a set duration and then stops the robot, which is a more predictable behavior than a single, instantaneous publish."})}),"\n",(0,i.jsx)(n.h2,{id:"25-summary",children:"2.5 Summary"}),"\n",(0,i.jsx)(n.p,{children:"In this chapter, you learned how to architect and build a complete, modular, voice-controlled robotics system in ROS 2. You created a robust audio capture node, a sophisticated speech-to-text node using the OpenAI Whisper API with voice activity detection, and a command interpretation node to control the robot. This forms a powerful foundation for human-robot interaction. In the final chapter, we will replace the simple command interpreter with a much more powerful LLM-based planner, enabling the robot to understand complex, multi-step instructions."})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,t)=>{t.d(n,{R:()=>r,x:()=>a});var s=t(6540);const i={},o=s.createContext(i);function r(e){const n=s.useContext(o);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:r(e.components),s.createElement(o.Provider,{value:n},e.children)}}}]);