# Module 4: Vision-Language-Action (VLA)

This module integrates large language models (LLMs) with humanoid robotics for voice-driven autonomous actions.

## Documentation

*   Specification and Planning documents are available in the project's `specs/004-module-4-vla/` directory.

## Chapters

*   [Chapter 1: Voice-to-Action using OpenAI Whisper](01-chapter1.md)
*   [Chapter 2: Cognitive Planning with LLMs](02-chapter2.md)
*   [Chapter 3: Capstone Autonomous Humanoid Project](03-chapter3.md)
