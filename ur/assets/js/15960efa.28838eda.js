"use strict";(globalThis.webpackChunkhumanoid_robotics=globalThis.webpackChunkhumanoid_robotics||[]).push([[370],{6996:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>c,contentTitle:()=>r,default:()=>u,frontMatter:()=>a,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"advanced-topics/translate-your-site","title":"Module 4: Vision-Language-Action (VLA)","description":"Module 4 explores the cutting-edge field of Vision-Language-Action (VLA) systems, focusing on how Large Language Models (LLMs) can be used for humanoid robot control. You will learn:","source":"@site/docs/advanced-topics/translate-your-site.md","sourceDirName":"advanced-topics","slug":"/advanced-topics/translate-your-site","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/docs/advanced-topics/translate-your-site","draft":false,"unlisted":false,"editUrl":"https://github.com/spect-kit-plus/Hakathon_01/tree/main/Humanoid_Robotics/docs/advanced-topics/translate-your-site.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"Module 4: Vision-Language-Action (VLA)"},"sidebar":"tutorialSidebar","previous":{"title":"Module 3: NVIDIA Isaac AI and Perception","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/docs/advanced-topics/manage-docs-versions"},"next":{"title":"\u0645\u0627\u0688\u06cc\u0648\u0644 1: \u0631\u0648\u0628\u0648\u0679\u06a9 \u0627\u0639\u0635\u0627\u0628\u06cc \u0646\u0638\u0627\u0645 (ROS 2)","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/docs/modules/ros2/"}}');var i=o(4848),s=o(8453);const a={sidebar_position:2,title:"Module 4: Vision-Language-Action (VLA)"},r="Module 4: Vision-Language-Action (VLA)",c={},d=[];function l(e){const n={code:"code",h1:"h1",header:"header",li:"li",p:"p",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,i.jsx)(n.p,{children:"Module 4 explores the cutting-edge field of Vision-Language-Action (VLA) systems, focusing on how Large Language Models (LLMs) can be used for humanoid robot control. You will learn:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"VLA System Architecture:"})," Understand the integration of visual perception, natural language understanding, and robot action generation to create intelligent agents."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LLM-based Humanoid Control:"})," Explore techniques for translating natural language commands into robot actions and behaviors using advanced GPT models."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"OpenAI API Integration:"})," Integrate OpenAI's Whisper API for accurate speech-to-text conversion, enabling robots to understand spoken commands."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 and LLM Orchestration:"})," Use ",(0,i.jsx)(n.code,{children:"rclpy"})," to bridge the gap between ROS 2 robotic systems and LLM-based decision-making, allowing for dynamic and intelligent task execution."]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Python for LLM Integration:"})," Develop Python scripts to manage the flow of information between perception systems, LLMs, and robot actuators, especially in diverse simulation environments (Gazebo, Unity, Isaac Sim)."]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This module prepares you to develop humanoid robots that can understand and respond to human instructions in a highly intuitive and flexible manner."})]})}function u(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(l,{...e})}):l(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>a,x:()=>r});var t=o(6540);const i={},s=t.createContext(i);function a(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function r(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:a(e.components),t.createElement(s.Provider,{value:n},e.children)}}}]);